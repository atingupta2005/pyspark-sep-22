{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9eab98a-5468-4bf5-a392-5ba3b669d1f4",
   "metadata": {},
   "source": [
    "# Apache Parquet Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45703b5d-2dad-43aa-8f7a-33fd570c7066",
   "metadata": {},
   "source": [
    "- Columnar file format\n",
    "- Provides optimizations to speed up queries\n",
    "- Far more efficient file format than CSV or JSON\n",
    "- Provides efficient data compression with enhanced performance to handle complex data in bulk.\n",
    "- Spark SQL provides support for both reading and writing Parquet files\n",
    "- Automatically capture the schema of the original data\n",
    "- Reduces data storage by 75% on average\n",
    "- Spark by default supports Parquet in its library hence we don’t need to add any dependency libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a53ac-5fac-40a4-a326-d758a980f769",
   "metadata": {},
   "source": [
    "## Apache Parquet Advantages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674516a-9300-40bf-9a00-da067b2439b4",
   "metadata": {},
   "source": [
    "- Reduces IO operations.\n",
    "- Fetches specific columns that you need to access.\n",
    "- Consumes less space.\n",
    "- Support type-specific encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cb0b5-d519-47b5-a887-4fcdd204b1b9",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa24b279-8107-47cd-a4ac-47941253722a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5cf627-7b72-4a0a-8946-edf028b8b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75395073-4e5e-4a32-89e2-8cbc06fae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "data = ((\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1))\n",
    "\n",
    "columns = (\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8cf223-0017-49c2-a6bd-681716646d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c168fd-6a2c-4003-a281-aa53a40cdfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4a69e-7c83-42a9-898e-dff847772287",
   "metadata": {},
   "source": [
    "## Spark Write DataFrame to Parquet file format\n",
    "    - Writing Spark DataFrame to Parquet format preserves the column names and data types\n",
    "    - All columns are automatically converted to be nullable for compatibility reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4018051-79ee-4bca-a1f4-a154eeb11b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/D:/22-Trngs/2-Confirmed/5-PySpark-56-hours-Geet/GH/Labs/Day3-Day4/data/people.parquet already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/people.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py:885\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m--> 885\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/D:/22-Trngs/2-Confirmed/5-PySpark-56-hours-Geet/GH/Labs/Day3-Day4/data/people.parquet already exists."
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aed516f-dc31-4116-89fd-3d303ae96d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\\data\\people.parquet\n",
      "\n",
      "07-05-2022  13:59    <DIR>          .\n",
      "07-05-2022  13:59    <DIR>          ..\n",
      "07-05-2022  13:59                24 .part-00000-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet.crc\n",
      "07-05-2022  13:59                24 .part-00001-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet.crc\n",
      "07-05-2022  13:59                24 .part-00002-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet.crc\n",
      "07-05-2022  13:59                24 .part-00003-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet.crc\n",
      "07-05-2022  13:59                 8 ._SUCCESS.crc\n",
      "07-05-2022  13:59             1,683 part-00000-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet\n",
      "07-05-2022  13:59             1,690 part-00001-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet\n",
      "07-05-2022  13:59             1,710 part-00002-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet\n",
      "07-05-2022  13:59             1,708 part-00003-c2417797-c2db-42af-9be4-f2b02c94ddb3-c000.snappy.parquet\n",
      "07-05-2022  13:59                 0 _SUCCESS\n",
      "              10 File(s)          6,895 bytes\n",
      "               2 Dir(s)  1,789,914,062,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir \"data/people.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77daa15f-bca7-466f-83c2-a0556786a06d",
   "metadata": {},
   "source": [
    "## Spark Read Parquet file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c484754-4e9c-4ccf-b7b8-811ba1f1c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqDF = spark.read.parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00477f50-f91d-4376-aac5-1043c9234c32",
   "metadata": {},
   "source": [
    "## Append to existing Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbc7a67-88e9-4136-a8d7-1d42eb965a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('append').parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433cebaa-75d6-4a0c-9e0c-5c83e6857373",
   "metadata": {},
   "source": [
    "## Using SQL queries on Parquet\n",
    "- We can also create a temporary view on Parquet files and then use it in Spark SQL statements\n",
    "- This temporary table would be available until the SparkContext present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960737b7-f2f2-4114-a941-516411cc53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438fda53-f644-4a7d-840a-5a618fd7e3fc",
   "metadata": {},
   "source": [
    "- Above predicate on spark parquet file does the file scan which is performance bottleneck like table scan on a traditional database\n",
    "- We should use partitioning in order to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a8e87-db1f-4b5a-b833-892601c4b9a2",
   "metadata": {},
   "source": [
    "## Spark parquet partition – Improving performance\n",
    "- Partitioning is a feature of many databases and data processing frameworks and it is key to make jobs work at scale\n",
    "- We can do a parquet file partition using spark partitionBy() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff0d067-acb5-4ea4-bf7d-58dab758476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"gender\",\"salary\").parquet(\"data/people2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c659e-bc58-4859-bff7-a2e3eeeed81b",
   "metadata": {},
   "source": [
    "- Parquet Partition creates a folder hierarchy for each spark partition\n",
    "- We have mentioned the first partition as gender followed by salary hence, it creates a salary folder inside the gender folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7457fe56-2dd7-4a42-8c5a-d1c52ed1fc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\\data\\people2.parquet\\gender=F\n",
      "\n",
      "13-05-2022  09:40    <DIR>          .\n",
      "13-05-2022  09:40    <DIR>          ..\n",
      "13-05-2022  09:40    <DIR>          salary=-1\n",
      "13-05-2022  09:40    <DIR>          salary=4000\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  1,789,914,001,408 bytes free\n",
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\\data\\people2.parquet\\gender=M\n",
      "\n",
      "13-05-2022  09:40    <DIR>          .\n",
      "13-05-2022  09:40    <DIR>          ..\n",
      "13-05-2022  09:40    <DIR>          salary=3000\n",
      "13-05-2022  09:40    <DIR>          salary=4000\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  1,789,914,001,408 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir people2.parquet\n",
    "! dir \"data/people2.parquet\\gender=F\"\n",
    "! dir \"data/people2.parquet\\gender=M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f597552-07b6-4a1d-96ff-1d6e662d3dd1",
   "metadata": {},
   "source": [
    "## Read from partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba0f529-a1d2-4338-856c-c500072537e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqDF = spark.read.parquet(\"data/people2.parquet\")\n",
    "parqDF.createOrReplaceTempView(\"Table2\")\n",
    "df = spark.sql(\"select * from Table2  where gender='M' and salary >= 4000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4a14a69-8a37-48f4-a3c6-76bcca130fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eaaccf-f791-4d07-91fd-4f08a159d551",
   "metadata": {},
   "source": [
    "- The execution of this query is significantly faster than the query without partition\n",
    "- It filters the data first on gender and then applies filters on salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623d04b-5571-49b3-ad14-dd695117d0e8",
   "metadata": {},
   "source": [
    "## Spark Read a specific Parquet partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e109e59-2fb1-4ce7-9555-2246a98009f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the data from the gender partition value “M”.\n",
    "parqDF = spark.read.parquet(\"data/people2.parquet/gender=M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba7481e3-01de-465c-bfb1-66ca6329913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|  dob|salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|  Robert |          |Williams|42114|  4000|\n",
      "| Michael |      Rose|        |40288|  4000|\n",
      "|   James |          |   Smith|36636|  3000|\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parqDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
