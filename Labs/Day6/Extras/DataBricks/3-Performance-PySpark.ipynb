{"cells":[{"cell_type":"markdown","source":["# Spark Performance Tuning & Best Practices"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2113c0d6-8f88-4e28-bc1c-7bc58f2e2d76"}}},{"cell_type":"markdown","source":["## Use DataFrame/Dataset over RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ff45988-ffc0-444f-8eac-13094e268cbe"}}},{"cell_type":"markdown","source":["- For Spark jobs, prefer using Dataset/DataFrame over RDD as Dataset and DataFrame’s includes several optimization modules to improve the performance of the Spark workloads\n- In PySpark use, DataFrame over RDD as Dataset’s are not supported in PySpark applications."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a42f7d33-d04d-465c-bb2f-b410af2df05b"}}},{"cell_type":"markdown","source":["- Spark RDD is a building block of Spark programming, even when we use DataFrame/Dataset, Spark internally uses RDD to execute operations/queries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"494304af-fd6b-4136-9b7f-effb739e971c"}}},{"cell_type":"markdown","source":["## Why RDD is slow?\n- Using RDD directly leads to performance issues as Spark doesn’t know how to apply the optimization techniques"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79385569-68d3-497a-b386-69355af320e7"}}},{"cell_type":"markdown","source":["## Use coalesce() over repartition()\n- When you want to reduce the number of partitions prefer using coalesce()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"237e9cff-6e24-4640-9214-9081c05bf871"}}},{"cell_type":"markdown","source":["## Use Parquet data format’s"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5bbb04d-7540-47a0-bee6-c8d02396dc6e"}}},{"cell_type":"markdown","source":["## Avoid UDF’s (User Defined Functions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b33d367f-d43d-4b6a-b847-8d3c9866ac3d"}}},{"cell_type":"markdown","source":["- Try to avoid Spark/PySpark UDF’s at any cost and use when existing Spark built-in functions are not available for use"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"930874af-6013-4b7e-a233-260c74d111e5"}}},{"cell_type":"markdown","source":["## Persisting data\n- Using persist() method, Spark provides an optimization mechanism to store the intermediate computation of a Spark DataFrame so they can be reused in subsequent actions.\n- Well, suppose you have written a few transformations to be performed on an RDD. \n- Now each time you call an action on the RDD, Spark recomputes the RDD and all its dependencies. \n- This can turn out to be quite expensive."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8251f89-bb37-41d3-8137-6bcb68f097b0"}}},{"cell_type":"code","source":["l1 = [1, 2, 3, 4]\n\nrdd1 = sc.parallelize(l1)\nrdd2 = rdd1.map(lambda x: x*x)\nrdd3 = rdd2.map(lambda x: x+2)\n\n# When I call count(), all the transformations are performed and it takes 0.1 s to complete the task.\nprint(rdd3.count())\n\n# When I call collect(), again all the transformations are called and it still takes me 0.1 s to complete the task.\nprint(rdd3.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d583949-b3a4-4bb9-acac-3e497be6d107"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- So how do we get out of this vicious cycle? Persist!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c603310-89af-42d1-8ea8-4342ed919928"}}},{"cell_type":"code","source":["from pyspark import StorageLevel\n\n# By default cached to memory and disk\nrdd3.persist(StorageLevel.MEMORY_AND_DISK)\n\n# before rdd is persisted (It will be persisted on first action as below)\nprint(rdd3.count())\n\n\n# after rdd is persisted (After the previous action is executed)\nprint(rdd3.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1373a910-8e1e-479e-8c3f-667d2fd8683f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- In our previous code, all we have to do is persist in the final RDD.\n- This way when we first call an action on the RDD, the final data generated will be stored in the cluster.\n- Now, any subsequent use of action on the same RDD would be much faster as we had already stored the previous result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a53bd92f-c8a8-4f57-b219-4c243d6125c4"}}},{"cell_type":"markdown","source":["## Reduce expensive Shuffle operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19f56e80-9db8-4ee5-88fd-beaffff7913f"}}},{"cell_type":"markdown","source":["- Spark shuffling triggers when we perform certain transformation operations like gropByKey(), reducebyKey(), join()\n- Spark Shuffle is an expensive operation since it involves the following\n  - Disk I/O\n  - Involves data serialization and deserialization\n  - Network I/O"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d1e5ba8-5d28-4243-84ca-1a9e7d035e3f"}}},{"cell_type":"markdown","source":["- DataFrame increases the partition number to 200 automatically when Spark operation performs data shuffling (join(), aggregation functions).\n- You can change this default shuffle partition value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9b9f2fc-8fa7-4876-8cd2-77c52ca1b651"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\",100)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a788bbe6-89d9-4cfb-81ca-15790ebe2004"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- In Spark 3.x, we have a newly added feature of adaptive query Execution.\n- When spark.sql.adaptive.enabled settled as true and spark.sql.adaptive.coalescePartitions.enabled settled as true, then the number of shuffle partitions can be updated by spark dynamically."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e46bbba9-7f65-4057-be33-d0d18476ee5a"}}},{"cell_type":"markdown","source":["## Getting the right size of the shuffle partition\n- Based on your dataset size, number of cores, and memory, Spark shuffling can benefit or harm your jobs\n- When you dealing with less amount of data, you should typically reduce the shuffle partitions otherwise you will end up with many partitioned files with a fewer number of records in each partition. which results in running many tasks with lesser data to process.\n- On another hand, when you have too much data and have less number of partitions results in fewer longer running tasks, and sometimes you may also get out of memory error.\n- Getting the right size of the shuffle partition is always tricky and takes many runs with different values to achieve the optimized number.\n- This is one of the key properties to look for when you have performance issues on Spark jobs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf9bdeb0-6db1-4f23-beb4-d3ccfe890fe2"}}},{"cell_type":"markdown","source":["## Don’t Collect Data\n- When we call the collect action, the result is returned to the driver node\n- If you are working with huge amounts of data, then the driver node might easily run out of memory.\n- One great way to escape is by using the take() action. It scans the first partition it finds and returns the result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8e7f89d-c601-44f0-9208-fea3f82541e9"}}},{"cell_type":"markdown","source":["## Aggregate with Accumulators"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f04bd6b2-f863-4b82-b261-0c2eab537199"}}},{"cell_type":"markdown","source":["- Suppose you want to aggregate some value. This can be done with simple programming using a variable for a counter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"833d3f2e-a1b7-4299-b1e7-702398342f42"}}},{"cell_type":"code","source":["file = sc.textFile(\"/FileStore/tables/log.txt\")\n\n# variable counter\nwarningCount = 0\n\ndef extractWarning(line):\n    global warningCount\n    if (\"WARNING\" in line):\n        warningCount +=1\n\nlines = file.flatMap(lambda x: x.split(\",\"))\nlines.foreach(extractWarning)\n\n# output variable\nwarningCount"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1da77ff1-d462-47a5-b64b-bab51e99a664"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### But there is a caveat here.\n\n- When we try to view the result on the driver node, then we get a 0 value\n- This is because when the code is implemented on the worker nodes, the variable becomes local to the node\n- This means that the updated value is not sent back to the driver node. To overcome this problem, we use accumulators."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4e3d76c-6019-4410-ab15-09927496c2f5"}}},{"cell_type":"markdown","source":["- Accumulators have shared variables provided by Spark\n- They are used for associative and commutative tasks\n- For example, if you want to count the number of blank lines in a text file or determine the amount of corrupted data then accumulators can turn out to be very helpful."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cd7e293-fc2f-4b3f-8718-74309b3b21cf"}}},{"cell_type":"code","source":["file = sc.textFile(\"/FileStore/tables/log.txt\")\n\n# accumulator\nwarningCount = sc.accumulator(0)\n\ndef extractWarning(line):\n    global warningCount\n    if (\"WARNING\" in line):\n        warningCount +=1\n\nlines = file.flatMap(lambda x: x.split(\",\"))\nlines.foreach(extractWarning)\n\n# accumulator value\nwarningCount.value  # output 4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb6d6cfe-29ae-4db9-a85e-b52311d105f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["- One thing to be remembered when working with accumulators is that worker nodes can only write to accumulators\n- But only the driver node can read the value."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e23c9605-2b99-47e1-a9b8-2fa7d222f9ac"}}},{"cell_type":"markdown","source":["## Broadcast Large Variables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecc48def-af9b-45ba-aad0-263320f89fe5"}}},{"cell_type":"markdown","source":["- Just like accumulators, Spark has another shared variable called the Broadcast variable\n- They are only used for reading purposes that get cached in all the worker nodes in the cluster\n- This comes in handy when you have to send a large look-up table to all nodes.\n\n- Assume a file containing data containing the shorthand code for countries (like IND for India) with other kinds of information\n- You have to transform these codes to the country name\n- This is where Broadcast variables come in handy using which we can cache the lookup tables in the worker nodes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4dd2e525-4450-4ad8-b477-1ae25528a434"}}},{"cell_type":"code","source":["# lookup\ncountry = {\"IND\":\"India\",\"USA\":\"United States of America\",\"SA\":\"South Africa\"}\n\n# broadcast\nbroadcaster = sc.broadcast(country)\n\n# data\nuserData = [(\"Johnny\",\"USA\"),(\"Faf\",\"SA\"),(\"Sachin\",\"IND\")]\n\n# create rdd\nrdd_data = sc.parallelize(userData)\n\n# use broadcast variable\ndef convert(code):\n    return broadcaster.value[code]\n\n# transformation\noutput = rdd_data.map(lambda x: (x[0], convert(x[1])))\n\n# action\noutput.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1deef94-99a3-4142-b0b3-06a0ec021e1a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Be shrewd with Partitioning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ec9c87b-4af5-4d07-96a1-20c282410a90"}}},{"cell_type":"markdown","source":["- One of the cornerstones of Spark is its ability to process data in a parallel fashion. \n- Spark splits data into several partitions, each containing some subset of the complete data. \n- For example, if a dataframe contains 10,000 rows and there are 10 partitions, then each partition will have 1000 rows.\n- The number of partitions in the cluster depends on the number of cores in the cluster and is controlled by the driver node. \n- When Spark runs a task, it is run on a single partition in the cluster"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2fad54c-b253-44a6-930b-61756091d2ed"}}},{"cell_type":"markdown","source":["- However, this number is adjustable and should be adjusted for better optimization.\n- Choose too few partitions, you have a number of resources sitting idle.\n- Choose too many partitions, you have a large number of small partitions shuffling data frequently, which can become highly inefficient.\n- So what’s the right number?\n- According to Spark, 128 MB is the maximum number of bytes you should pack into a single partition.\n- So, if we have 128000 MB of data, we should have 1000 partitions. But this number is not rigid as we will see in the next tip."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b75e60b8-36e7-430f-ba9f-58572a90a6f8"}}},{"cell_type":"markdown","source":["## Monitoring of Job Stages\n- Most of the developers write and execute the code, but monitoring of Job tasks is essential. This monitoring is best achieved by managing DAG and reducing the stages"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f31920d3-fb43-4381-bf81-2fb8b419a4c9"}}},{"cell_type":"markdown","source":["## Use Predicate Pushdown\n- Predicate push down is another feature of Spark and Parquet that can improve query performance by reducing the amount of data read from Parquet files.\n- Push down means the filters are pushed to the source as opposed to being brought into Spark\n-"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60411bea-8502-4888-9716-fa8d5bb8b05c"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Performance-PySpark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4326754976675683}},"nbformat":4,"nbformat_minor":0}
