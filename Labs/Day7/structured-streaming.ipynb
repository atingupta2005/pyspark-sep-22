{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/11 10:59:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/09/11 10:59:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retailDataDay = \"data/retail-data/by-day/\"\n",
    "retailDataDaySmall = \"data/retail-data/by-day/2011-12-06.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note remove all the data from data/retail-data/by-day/ and keep only 1 csv file - 2011-12-06.csv\n",
    " - Need to remove manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark\\\n",
    ".read\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(retailDataDaySmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxFilesPerTrigger option simply specifies the number of files we should read in at once\n",
    "# This is to make our demonstration more “streaming,” and in a production scenario this would probably be omitted\n",
    "\n",
    "streamingDataFrame = spark\\\n",
    ".readStream\\\n",
    ".schema(staticSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(retailDataDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomer = streamingDataFrame\\\n",
    ".selectExpr(\\\n",
    "           \"CustomerId\",\n",
    "           \"(UnitPrice * Quantity) as total_cost\",\\\n",
    "           \"InvoiceDate\")\\\n",
    ".groupBy(\\\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/11 10:59:44 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c947e4d4-5ff2-43ca-8943-9e2c5732f8ae. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f69c88f3fa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomer.writeStream\\\n",
    ".format(\"memory\")\\\n",
    ".queryName(\"customer_purchases\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *\n",
    "from customer_purchases\n",
    "order by 'sum(total_cost)' desc\"\"\")\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/11 10:59:46 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e05ed3bf-aaa3-4e32-a20c-6d0fef4c19ac. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "[Stage 3:=>                                                       (5 + 1) / 200]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f69c88f38e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+--------------------+-------------------+\n",
      "|CustomerId|              window|    sum(total_cost)|\n",
      "+----------+--------------------+-------------------+\n",
      "|   14071.0|[2011-12-06 00:00...|             154.58|\n",
      "|   14543.0|[2011-12-06 00:00...|             -32.05|\n",
      "|   12417.0|[2011-12-06 00:00...|             369.73|\n",
      "|   14503.0|[2011-12-06 00:00...|             337.75|\n",
      "|   18005.0|[2011-12-06 00:00...|             109.75|\n",
      "|   15189.0|[2011-12-06 00:00...|             348.72|\n",
      "|   15622.0|[2011-12-06 00:00...|-381.95000000000005|\n",
      "|   12839.0|[2011-12-06 00:00...|               -4.5|\n",
      "|   17861.0|[2011-12-06 00:00...| 438.58000000000015|\n",
      "|   17389.0|[2011-12-06 00:00...|            5134.88|\n",
      "|   17490.0|[2011-12-06 00:00...|              -40.4|\n",
      "|   16133.0|[2011-12-06 00:00...| 309.21999999999997|\n",
      "|   13362.0|[2011-12-06 00:00...|              -5.25|\n",
      "|   17841.0|[2011-12-06 00:00...|  974.0100000000001|\n",
      "|   14298.0|[2011-12-06 00:00...|            -664.86|\n",
      "|   14140.0|[2011-12-06 00:00...| 394.06000000000006|\n",
      "|   15023.0|[2011-12-06 00:00...|  691.6400000000003|\n",
      "|   16554.0|[2011-12-06 00:00...|            -102.45|\n",
      "|   15822.0|[2011-12-06 00:00...| 445.27000000000004|\n",
      "|   12682.0|[2011-12-06 00:00...|  636.6300000000001|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/11 11:00:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 33800 milliseconds\n",
      "22/09/11 11:01:07 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 27603 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------+--------------------+-------------------+\n",
      "|CustomerId|              window|    sum(total_cost)|\n",
      "+----------+--------------------+-------------------+\n",
      "|   15136.0|[2011-12-07 00:00...|              256.9|\n",
      "|   13743.0|[2011-12-07 00:00...|             275.76|\n",
      "|   14071.0|[2011-12-06 00:00...|             154.58|\n",
      "|   14543.0|[2011-12-06 00:00...|             -32.05|\n",
      "|   15525.0|[2011-12-07 00:00...| 299.78999999999996|\n",
      "|   15568.0|[2011-12-07 00:00...| 335.77000000000004|\n",
      "|   12826.0|[2011-12-07 00:00...|              142.6|\n",
      "|   12417.0|[2011-12-06 00:00...|             369.73|\n",
      "|   17490.0|[2011-12-07 00:00...|-42.760000000000005|\n",
      "|   14503.0|[2011-12-06 00:00...|             337.75|\n",
      "|   13756.0|[2011-12-07 00:00...|  846.3999999999999|\n",
      "|   12912.0|[2011-12-07 00:00...| 187.92000000000002|\n",
      "|   18005.0|[2011-12-06 00:00...|             109.75|\n",
      "|   15189.0|[2011-12-06 00:00...|             348.72|\n",
      "|   15622.0|[2011-12-06 00:00...|-381.95000000000005|\n",
      "|   15195.0|[2011-12-07 00:00...|             3861.0|\n",
      "|   14708.0|[2011-12-07 00:00...|             425.19|\n",
      "|   13870.0|[2011-12-07 00:00...|             152.75|\n",
      "|   12839.0|[2011-12-06 00:00...|               -4.5|\n",
      "|   17091.0|[2011-12-07 00:00...| 450.80000000000007|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/11 11:01:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 20907 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------+--------------------+-------------------+\n",
      "|CustomerId|              window|    sum(total_cost)|\n",
      "+----------+--------------------+-------------------+\n",
      "|   15136.0|[2011-12-07 00:00...|              256.9|\n",
      "|   13743.0|[2011-12-07 00:00...|             275.76|\n",
      "|   17144.0|[2011-12-08 00:00...| 388.68000000000006|\n",
      "|   14071.0|[2011-12-06 00:00...|             154.58|\n",
      "|   14543.0|[2011-12-06 00:00...|             -32.05|\n",
      "|   13408.0|[2011-12-08 00:00...| 445.84000000000003|\n",
      "|   15525.0|[2011-12-07 00:00...| 299.78999999999996|\n",
      "|   16891.0|[2011-12-08 00:00...|             115.95|\n",
      "|   15568.0|[2011-12-07 00:00...| 335.77000000000004|\n",
      "|   12826.0|[2011-12-07 00:00...|              142.6|\n",
      "|   12417.0|[2011-12-06 00:00...|             369.73|\n",
      "|   13756.0|[2011-12-07 00:00...|  846.3999999999999|\n",
      "|   17490.0|[2011-12-07 00:00...|-42.760000000000005|\n",
      "|   14503.0|[2011-12-06 00:00...|             337.75|\n",
      "|   12912.0|[2011-12-07 00:00...| 187.92000000000002|\n",
      "|   17673.0|[2011-12-08 00:00...| 105.57000000000001|\n",
      "|   18005.0|[2011-12-06 00:00...|             109.75|\n",
      "|   13883.0|[2011-12-08 00:00...|             -36.15|\n",
      "|   13263.0|[2011-12-08 00:00...| 207.34999999999994|\n",
      "|   15189.0|[2011-12-06 00:00...|             348.72|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/11 11:02:05 ERROR MicroBatchExecution: Query customer_purchases [id = 44e0dc9b-b440-42c5-abec-a352becbbc17, runId = 50e7b20a-b5ba-4b33-b82c-b9b92a50100a] terminated with error\n",
      "org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\n",
      "Exchange hashpartitioning(CustomerID#12513, window#58, 200), true, [id=#808]\n",
      "+- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#12513)) AS CustomerID#12513, window#58], functions=[partial_sum(total_cost#48)], output=[CustomerID#12513, window#58, sum#88])\n",
      "   +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) as double) = (cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) THEN (CEIL((cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) + 1) ELSE CEIL((cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) END + 0) - 1) * 86400000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) as double) = (cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) THEN (CEIL((cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) + 1) ELSE CEIL((cast((precisetimestampconversion(cast(InvoiceDate#12511 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) END + 0) - 1) * 86400000000) + 86400000000), LongType, TimestampType)) AS window#58, CustomerID#12513, (UnitPrice#12512 * cast(Quantity#12510 as double)) AS total_cost#48]\n",
      "      +- *(1) Filter (isnotnull(InvoiceDate#12511) AND isnotnull(cast(InvoiceDate#12511 as timestamp)))\n",
      "         +- FileScan csv [Quantity#12510,InvoiceDate#12511,UnitPrice#12512,CustomerID#12513] Batched: false, DataFilters: [isnotnull(InvoiceDate#12511), isnotnull(cast(InvoiceDate#12511 as timestamp))], Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/u50/Day7/data/retail-data/by-day/2011-12-04.csv], PartitionFilters: [], PushedFilters: [IsNotNull(InvoiceDate)], ReadSchema: struct<Quantity:int,InvoiceDate:string,UnitPrice:double,CustomerID:double>\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreRestoreExec.doExecute(statefulOperators.scala:236)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.doExecute(statefulOperators.scala:294)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:363)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:361)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:322)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:329)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:45)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2938)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2938)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:576)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:571)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:571)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:111)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1471)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:398)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:389)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:472)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:133)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:64)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:64)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:81)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\t... 89 more\n",
      "22/09/11 11:02:05 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@2e4f0207 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7b33df67[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "Exception in thread \"stream execution thread for customer_purchases [id = 44e0dc9b-b440-42c5-abec-a352becbbc17, runId = 50e7b20a-b5ba-4b33-b82c-b9b92a50100a]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:108)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:439)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$2(StreamExecution.scala:382)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:363)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:167)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:548)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:552)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 7 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----------+--------------------+-------------------+\n",
      "|CustomerId|              window|    sum(total_cost)|\n",
      "+----------+--------------------+-------------------+\n",
      "|   15136.0|[2011-12-07 00:00...|              256.9|\n",
      "|   13743.0|[2011-12-07 00:00...|             275.76|\n",
      "|   17114.0|[2011-12-04 00:00...|  90.89000000000001|\n",
      "|   17144.0|[2011-12-08 00:00...| 388.68000000000006|\n",
      "|   14071.0|[2011-12-06 00:00...|             154.58|\n",
      "|   14543.0|[2011-12-06 00:00...|             -32.05|\n",
      "|   13408.0|[2011-12-08 00:00...| 445.84000000000003|\n",
      "|   15525.0|[2011-12-07 00:00...| 299.78999999999996|\n",
      "|      null|[2011-12-04 00:00...|  4245.470000000001|\n",
      "|   16891.0|[2011-12-08 00:00...|             115.95|\n",
      "|   15568.0|[2011-12-07 00:00...| 335.77000000000004|\n",
      "|   16988.0|[2011-12-04 00:00...|             126.12|\n",
      "|   12826.0|[2011-12-07 00:00...|              142.6|\n",
      "|   12417.0|[2011-12-06 00:00...|             369.73|\n",
      "|   13756.0|[2011-12-07 00:00...|  846.3999999999999|\n",
      "|   17490.0|[2011-12-07 00:00...|-42.760000000000005|\n",
      "|   14503.0|[2011-12-06 00:00...|             337.75|\n",
      "|   12912.0|[2011-12-07 00:00...| 187.92000000000002|\n",
      "|   14720.0|[2011-12-04 00:00...|             262.68|\n",
      "|   17673.0|[2011-12-08 00:00...| 105.57000000000001|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/11 11:02:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 18012 milliseconds\n",
      "22/09/11 11:03:57 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 17418 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----------+--------------------+-------------------+\n",
      "|CustomerId|              window|    sum(total_cost)|\n",
      "+----------+--------------------+-------------------+\n",
      "|   15136.0|[2011-12-07 00:00...|              256.9|\n",
      "|   13743.0|[2011-12-07 00:00...|             275.76|\n",
      "|   17114.0|[2011-12-04 00:00...|  90.89000000000001|\n",
      "|   17144.0|[2011-12-08 00:00...| 388.68000000000006|\n",
      "|   14071.0|[2011-12-06 00:00...|             154.58|\n",
      "|   14543.0|[2011-12-06 00:00...|             -32.05|\n",
      "|   13408.0|[2011-12-08 00:00...| 445.84000000000003|\n",
      "|      null|[2011-12-04 00:00...|  4245.470000000001|\n",
      "|   15525.0|[2011-12-07 00:00...| 299.78999999999996|\n",
      "|   16891.0|[2011-12-08 00:00...|             115.95|\n",
      "|   16988.0|[2011-12-04 00:00...|             126.12|\n",
      "|   12826.0|[2011-12-07 00:00...|              142.6|\n",
      "|   15568.0|[2011-12-07 00:00...| 335.77000000000004|\n",
      "|   12417.0|[2011-12-06 00:00...|             369.73|\n",
      "|   13756.0|[2011-12-07 00:00...|  846.3999999999999|\n",
      "|   17490.0|[2011-12-07 00:00...|-42.760000000000005|\n",
      "|   14503.0|[2011-12-06 00:00...|             337.75|\n",
      "|   12912.0|[2011-12-07 00:00...| 187.92000000000002|\n",
      "|   14720.0|[2011-12-04 00:00...|             262.68|\n",
      "|   17315.0|[2011-12-09 00:00...|               -7.5|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomer.writeStream.trigger(processingTime='10 seconds')\\\n",
    "  .format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the files are processed, copy 1 CSV file each time manually to data/retail-data/by-day/\n",
    "- Notice that processing is again started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
