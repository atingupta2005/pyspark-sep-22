{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e7a3f6-40ec-4756-b83b-2aa6262f3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74677a-38ea-4551-b111-81c3c721859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013ef44-1bdb-43e2-a057-980aea929519",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8834c-838a-43ee-a2db-2aadb32fcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e52132-4645-4c0d-b714-5e80e6ef4141",
   "metadata": {},
   "source": [
    "## Update table data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776220b-dafe-49b5-bd0a-9d406bad8946",
   "metadata": {},
   "source": [
    "### Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8c5e9-27b3-4155-b43e-113084358b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(5, 10)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ffe86-f8a5-4249-8a58-39ac1b06bc15",
   "metadata": {},
   "source": [
    "- If you read this table again, you should see only the values 5-9 you have added because you overwrote the previous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59b526-d8d0-42b8-a817-c305cc86eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b5333-ed24-4611-b6cf-a5c499e35946",
   "metadata": {},
   "source": [
    "## Conditional update without overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ce93c-74c2-4da3-bb56-f4d8291b384b",
   "metadata": {},
   "source": [
    "- Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f2c62-e0ce-4402-aa80-3ce660584396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32385126-5d81-45b3-a2ec-23fe5a5abc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update every even value by adding 100 to it\n",
    "deltaTable.update(\n",
    "  condition = expr(\"id % 2 == 0\"),\n",
    "  set = { \"id\": expr(\"id + 100\") })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d472977-4497-4c11-b4d2-87d1a7f9fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete every even value\n",
    "deltaTable.delete(condition = expr(\"id % 2 == 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f322ab2-65f8-4066-9903-c97b4f31ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert (merge) new data\n",
    "newData = spark.range(0, 20)\n",
    "deltaTable.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    newData.alias(\"newData\"),\n",
    "    \"oldData.id = newData.id\") \\\n",
    "  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n",
    "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n",
    "  .execute()\n",
    "\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2dc16-f3c1-44db-85f6-0546ab05c957",
   "metadata": {},
   "source": [
    "## Read older versions of data using time travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12190d0-5102-4f3d-8a9d-e74d090c6a1c",
   "metadata": {},
   "source": [
    "- You can query previous snapshots of your Delta table by using time travel.\n",
    "- If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1df2ca-0f87-42f3-a5bc-42e828e03d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72771c-c48f-4e67-a06f-02f09031faff",
   "metadata": {},
   "source": [
    "- You should see the first set of data, from before you overwrote it\n",
    "- Time travel takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table\n",
    "- Removing the version 0 option (or specifying version 1) would let you see the newer data again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97032221-9b5a-4072-9039-2c14cef62240",
   "metadata": {},
   "source": [
    "## Write a stream of data to a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d4d5f-7b65-419c-8ee8-93d8cce08f5c",
   "metadata": {},
   "source": [
    "- You can also write to a Delta table using Structured Streaming\n",
    "- The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table\n",
    "- By default, streams run in append mode, which adds new records to the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed228bf6-7e70-4428-9682-2aa71859d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDf = spark.readStream.format(\"rate\").load()\n",
    "stream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d55246-ecfc-4d36-85fa-bce4bff052fa",
   "metadata": {},
   "source": [
    "- While the stream is running, you can read the table using the earlier commands.\n",
    "- Note: \n",
    "    - If youâ€™re running this in a shell, you may see the streaming task progress, which make it hard to type commands in that shell.\n",
    "    - It may be useful to start another shell in a new terminal for querying the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1deddb9-595b-47e4-8ade-f7d475862585",
   "metadata": {},
   "source": [
    "- You can stop the stream by running stream.stop() in the same terminal that started the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed07fc3-fdce-4535-b868-df5f4d5c7717",
   "metadata": {},
   "source": [
    "## Read a stream of changes from a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd6f02-7b8b-4680-afa1-6ece1ec8cc8d",
   "metadata": {},
   "source": [
    "- While the stream is writing to the Delta table, you can also read from that table as streaming source\n",
    "- For example, you can start another streaming query that prints all the changes made to the Delta table\n",
    "- You can specify which version Structured Streaming should start from by providing the startingVersion or startingTimestamp option to get changes from that point onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4897be7-0f7f-4251-bc50-e7a00b3cc57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o31.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: delta. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:156)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:209)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stream2 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/delta-table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\streaming.py:452\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(path\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the path is provided for stream, it needs to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    451\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-empty string. List of paths are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o31.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: delta. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:156)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:209)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "stream2 = spark.readStream.format(\"delta\").load(\"/tmp/delta-table\").writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2b2ae-e4b5-4593-be10-adf2e2895e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
